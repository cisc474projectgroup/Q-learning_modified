{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q-learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2facSkMSsMq"
      },
      "source": [
        "import random\n",
        "import copy\n",
        "from collections import defaultdict\n",
        "from collections import deque\n",
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "import unittest\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "import os\n",
        "import math\n",
        "import argparse\n",
        "from agent import Q, Agent, Trainer\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pA-wWZKtSwMw"
      },
      "source": [
        "class Q():\n",
        "\n",
        "    def __init__(self, n_actions, observation_space, bin_size, low_bound=None, high_bound=None, initial_mean=0.0, initial_std=0.0):\n",
        "        self.n_actions = n_actions\n",
        "        self._observation_dimension = 1\n",
        "        for d in observation_space.shape:\n",
        "            self._observation_dimension *= d\n",
        "\n",
        "        self._bin_sizes = bin_size if isinstance(bin_size, list) else [bin_size] * self._observation_dimension\n",
        "        self._dimension_bins = []\n",
        "        for i, low, high in self._low_high_iter(observation_space, low_bound, high_bound):\n",
        "            b_size = self._bin_sizes[i]\n",
        "            bins = self._make_bins(low, high, b_size)\n",
        "            self._dimension_bins.append(bins)\n",
        "\n",
        "        # if we encounter the new observation, we initialize action evaluations\n",
        "        self.table = defaultdict(lambda: initial_std * np.random.randn(self.n_actions) + initial_mean)\n",
        "    \n",
        "    @classmethod\n",
        "    def _make_bins(cls, low, high, bin_size):\n",
        "        bins = np.arange(low, high, (float(high) - float(low)) / (bin_size - 2))  # exclude both ends\n",
        "        if min(bins) < 0 and 0 not in bins:\n",
        "            bins = np.sort(np.append(bins, [0]))  # 0 centric bins\n",
        "        return bins\n",
        "    \n",
        "    @classmethod\n",
        "    def _low_high_iter(cls, observation_space, low_bound, high_bound):\n",
        "        lows = observation_space.low\n",
        "        highs = observation_space.high\n",
        "        for i in range(len(lows)):\n",
        "            low = lows[i]\n",
        "            if low_bound is not None:\n",
        "                _low_bound = low_bound if not isinstance(low_bound, list) else low_bound[i]\n",
        "                low = low if _low_bound is None else max(low, _low_bound)\n",
        "            \n",
        "            high = highs[i]\n",
        "            if high_bound is not None:\n",
        "                _high_bound = high_bound if not isinstance(high_bound, list) else high_bound[i]\n",
        "                high = high if _high_bound is None else min(high, _high_bound)\n",
        "            \n",
        "            yield i, low, high\n",
        "\n",
        "    def observation_to_state(self, observation):\n",
        "        state = 0\n",
        "        # caution: bin_size over 10 will not work accurately\n",
        "        unit = max(self._bin_sizes)\n",
        "        for d, o in enumerate(observation.flatten()):\n",
        "            state = state + np.digitize(o, self._dimension_bins[d]) * pow(unit, d)  # bin_size numeral system\n",
        "        return state\n",
        "    \n",
        "    def values(self, observation):\n",
        "        state = self.observation_to_state(observation)\n",
        "        return self.table[state]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAOqWFAmSzGe"
      },
      "source": [
        "class Agent():\n",
        "\n",
        "    def __init__(self, q, epsilon=0.05):\n",
        "        self.q = q\n",
        "        self.epsilon = epsilon\n",
        "    \n",
        "    def act(self, observation):\n",
        "        action = -1\n",
        "        if np.random.random() < self.epsilon:\n",
        "            action = np.random.choice(self.q.n_actions)\n",
        "        else:\n",
        "            action = np.argmax(self.q.values(observation))\n",
        "        \n",
        "        return action\n",
        "\n",
        "\n",
        "class Trainer():\n",
        "\n",
        "    def __init__(self, agent, gamma=0.95, learning_rate=0.1, learning_rate_decay=None, epsilon=0.05, epsilon_decay=None, max_step=-1):\n",
        "        self.agent = agent\n",
        "        self.gamma = gamma\n",
        "        self.learning_rate = learning_rate\n",
        "        self.learning_rate_decay = learning_rate_decay\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.max_step = max_step\n",
        "\n",
        "    def train(self, env, episode_count, render=False):\n",
        "        default_epsilon = self.agent.epsilon\n",
        "        self.agent.epsilon = self.epsilon\n",
        "        values = []\n",
        "        steps = deque(maxlen=100)\n",
        "        lr = self.learning_rate\n",
        "        for i in range(episode_count):\n",
        "            obs = env.reset()\n",
        "            step = 0\n",
        "            done = False\n",
        "            while not done:\n",
        "                if render:\n",
        "                    env.render()\n",
        "\n",
        "                action = self.agent.act(obs)\n",
        "                next_obs, reward, done, _ = env.step(action)\n",
        "\n",
        "                state = self.agent.q.observation_to_state(obs)\n",
        "                future = 0 if done else np.max(self.agent.q.values(next_obs))\n",
        "                value = self.agent.q.table[state][action]\n",
        "                self.agent.q.table[state][action] += lr * (reward + self.gamma * future - value)\n",
        "\n",
        "                obs = next_obs\n",
        "                values.append(value)\n",
        "                step += 1\n",
        "                if self.max_step > 0 and step > self.max_step:\n",
        "                    done = True\n",
        "            else:\n",
        "                mean = np.mean(values)\n",
        "                steps.append(step)\n",
        "                mean_step = np.mean(steps)\n",
        "                print(\"Episode {}: {}steps(avg{}). epsilon={:.3f}, lr={:.3f}, mean q value={:.2f}\".format(\n",
        "                    i, step, mean_step, self.agent.epsilon, lr, mean)\n",
        "                    )\n",
        "                \n",
        "                if self.epsilon_decay is not None:                \n",
        "                    self.agent.epsilon = self.epsilon_decay(self.agent.epsilon, i)\n",
        "                if self.learning_rate_decay is not None:\n",
        "                    lr = self.learning_rate_decay(lr, i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc1yDF_zTGsA"
      },
      "source": [
        "class TestCartPole(unittest.TestCase):\n",
        "\n",
        "    def test_make_bins(self):\n",
        "        env = gym.make(\"CartPole-v0\")\n",
        "        q = Q(env.action_space.n, env.observation_space, bin_size=7, low_bound=-3, high_bound=3)\n",
        "\n",
        "        bin_range = (-2, 3)\n",
        "        correct = np.arange(*bin_range).tolist()  # expected bins: ~-2, ~-1, ~0, ~1, ~2, ~3, (3~) = 7bin, 6 boundary\n",
        "        bins = q._make_bins(bin_range[0], bin_range[1], 7)\n",
        "        self.assertEqual(tuple(correct), tuple(bins))\n",
        "    \n",
        "    def test_make_bins_multi_sizes(self):\n",
        "        dummy_observation_space = Box(0, 6, (2,))\n",
        "        q = Q(4, dummy_observation_space, bin_size=[3, 5])\n",
        "        self.assertEqual(3 - 2, len(q._dimension_bins[0]))\n",
        "        self.assertEqual(5 - 2, len(q._dimension_bins[1]))\n",
        "\n",
        "    def test_make_bins_multi_bounds(self):\n",
        "        dummy_observation_space = Box(-3, 3, (2,))\n",
        "        q = Q(4, dummy_observation_space, bin_size=[3, 5], low_bound=[-2, -1], high_bound=[2, 1])\n",
        "        self.assertEqual(-2, q._dimension_bins[0][0])\n",
        "        self.assertEqual(-1, q._dimension_bins[1][0])\n",
        "        self.assertLess(q._dimension_bins[0][-1], 2)\n",
        "        self.assertLess(q._dimension_bins[1][-1], 1)\n",
        "\n",
        "    def test_observation_to_state(self):\n",
        "        dummy_observation_space = Box(-2, 3, (2,))\n",
        "        bin_size = 7\n",
        "        q = Q(4, dummy_observation_space, bin_size=bin_size, low_bound=-3, high_bound=3)\n",
        "\n",
        "        state = q.observation_to_state(np.array([-3, 1]))\n",
        "        self.assertEqual(state, 0 * bin_size ** 0 + 4 * bin_size ** 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOxpvyGWT4FX"
      },
      "source": [
        "RECORD_PATH = os.path.join(os.path.dirname(__file__), \"./upload\")\n",
        "\n",
        "\n",
        "def main(episodes, render, monitor):\n",
        "    env = gym.make(\"CartPole-v0\") \n",
        "\n",
        "    q = Q(\n",
        "        env.action_space.n, \n",
        "        env.observation_space, \n",
        "        bin_size=[3, 3, 8, 5],\n",
        "        low_bound=[None, -0.5, None, -math.radians(50)], \n",
        "        high_bound=[None, 0.5, None, math.radians(50)]\n",
        "        )\n",
        "    agent = Agent(q, epsilon=0.05)\n",
        "\n",
        "    learning_decay = lambda lr, t: max(0.1, min(0.5, 1.0 - math.log10((t + 1) / 25)))\n",
        "    epsilon_decay = lambda eps, t: max(0.01, min(1.0, 1.0 - math.log10((t + 1) / 25)))\n",
        "    trainer = Trainer(\n",
        "        agent, \n",
        "        gamma=0.99,\n",
        "        learning_rate=0.5, learning_rate_decay=learning_decay, \n",
        "        epsilon=1.0, epsilon_decay=epsilon_decay,\n",
        "        max_step=250)\n",
        "\n",
        "    if monitor:\n",
        "        env.monitor.start(RECORD_PATH)\n",
        "\n",
        "    trainer.train(env, episode_count=episodes, render=render)\n",
        "\n",
        "    if monitor:\n",
        "        env.monitor.close()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"train & run cartpole \")\n",
        "    parser.add_argument(\"--episode\", type=int, default=1000, help=\"episode to train\")\n",
        "    parser.add_argument(\"--render\", action=\"store_true\", help=\"render the screen\")\n",
        "    parser.add_argument(\"--monitor\", action=\"store_true\", help=\"monitor\")\n",
        "    parser.add_argument(\"--upload\", type=str, default=\"\", help=\"upload key to openai gym (training is not executed)\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.upload:\n",
        "        if os.path.isdir(RECORD_PATH):\n",
        "            gym.upload(RECORD_PATH, api_key=args.upload)\n",
        "    else:\n",
        "        main(args.episode, args.render, args.monitor)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}