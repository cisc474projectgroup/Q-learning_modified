{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q-learning_simplified.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRCMwG_EZgoV"
      },
      "source": [
        "#https://medium.com/swlh/using-q-learning-for-openais-cartpole-v1-4a216ef237df\n",
        "#source from \n",
        "#A extension on the project\n",
        "import numpy as np # used for arrays\n",
        "\n",
        "import gym # pull the environment\n",
        "\n",
        "import time # to get the time\n",
        "\n",
        "import math # needed for calculations\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython import display"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7u84uR59Jlbg"
      },
      "source": [
        "import math\n",
        "import gym\n",
        "from gym import spaces, logger\n",
        "from gym.utils import seeding\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class CartPoleEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Description:\n",
        "        A pole is attached by an un-actuated joint to a cart, which moves along\n",
        "        a frictionless track. The pendulum starts upright, and the goal is to\n",
        "        prevent it from falling over by increasing and reducing the cart's\n",
        "        velocity.\n",
        "    Source:\n",
        "        This environment corresponds to the version of the cart-pole problem\n",
        "        described by Barto, Sutton, and Anderson\n",
        "    Observation:\n",
        "        Type: Box(4)\n",
        "        Num     Observation               Min                     Max\n",
        "        0       Cart Position             -4.8                    4.8\n",
        "        1       Cart Velocity             -Inf                    Inf\n",
        "        2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
        "        3       Pole Angular Velocity     -Inf                    Inf\n",
        "    Actions:\n",
        "        Type: Discrete(2)\n",
        "        Num   Action\n",
        "        0     Push cart to the left\n",
        "        1     Push cart to the right\n",
        "        Note: The amount the velocity that is reduced or increased is not\n",
        "        fixed; it depends on the angle the pole is pointing. This is because\n",
        "        the center of gravity of the pole increases the amount of energy needed\n",
        "        to move the cart underneath it\n",
        "    Reward:\n",
        "        Reward is 1 for every step taken, including the termination step\n",
        "    Starting State:\n",
        "        All observations are assigned a uniform random value in [-0.05..0.05]\n",
        "    Episode Termination:\n",
        "        Pole Angle is more than 12 degrees.\n",
        "        Cart Position is more than 2.4 (center of the cart reaches the edge of\n",
        "        the display).\n",
        "        Episode length is greater than 200.\n",
        "        Solved Requirements:\n",
        "        Considered solved when the average return is greater than or equal to\n",
        "        195.0 over 100 consecutive trials.\n",
        "    \"\"\"\n",
        "\n",
        "    metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": 50}\n",
        "\n",
        "    def __init__(self):\n",
        "        self.gravity = 9.8\n",
        "        self.masscart = 1.0\n",
        "        self.masspole = 0.1\n",
        "        self.total_mass = self.masspole + self.masscart\n",
        "        self.length = 0.5  # actually half the pole's length\n",
        "        self.polemass_length = self.masspole * self.length\n",
        "        self.force_mag = 10.0\n",
        "        self.tau = 0.02  # seconds between state updates\n",
        "        self.kinematics_integrator = \"euler\"\n",
        "\n",
        "        # Angle at which to fail the episode\n",
        "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
        "        self.x_threshold = 2.4\n",
        "\n",
        "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
        "        # is still within bounds.\n",
        "        high = np.array(\n",
        "            [\n",
        "                self.x_threshold * 2,\n",
        "                np.finfo(np.float32).max,\n",
        "                self.theta_threshold_radians * 2,\n",
        "                np.finfo(np.float32).max,\n",
        "            ],\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "\n",
        "        self.action_space = spaces.Discrete(2)\n",
        "        self.observation_space = spaces.Box(-high, high)\n",
        "\n",
        "        self.seed()\n",
        "        self.viewer = None\n",
        "        self.state = None\n",
        "\n",
        "        self.steps_beyond_done = None\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def step(self, action):\n",
        "        err_msg = f\"{action!r} ({type(action)}) invalid\"\n",
        "        assert self.action_space.contains(action), err_msg\n",
        "\n",
        "        x, x_dot, theta, theta_dot = self.state\n",
        "\n",
        "        force = self.force_mag if action == 1 else -self.force_mag\n",
        "        costheta = math.cos(theta)\n",
        "        sintheta = math.sin(theta)\n",
        "\n",
        "        # For the interested reader:\n",
        "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
        "        temp = (\n",
        "            force + self.polemass_length * theta_dot ** 2 * sintheta\n",
        "        ) / self.total_mass\n",
        "        thetaacc = (self.gravity * sintheta - costheta * temp) / (\n",
        "            self.length * (4.0 / 3.0 - self.masspole * costheta ** 2 / self.total_mass)\n",
        "        )\n",
        "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
        "\n",
        "        if self.kinematics_integrator == \"euler\":\n",
        "            x = x + self.tau * x_dot\n",
        "            x_dot = x_dot + self.tau * xacc\n",
        "            theta = theta + self.tau * theta_dot\n",
        "            theta_dot = theta_dot + self.tau * thetaacc\n",
        "        else:  # semi-implicit euler\n",
        "            x_dot = x_dot + self.tau * xacc\n",
        "            x = x + self.tau * x_dot\n",
        "            theta_dot = theta_dot + self.tau * thetaacc\n",
        "            theta = theta + self.tau * theta_dot\n",
        "\n",
        "        self.state = (x, x_dot, theta, theta_dot)\n",
        "\n",
        "        done = bool(\n",
        "            x < -self.x_threshold\n",
        "            or x > self.x_threshold\n",
        "            or theta < -self.theta_threshold_radians\n",
        "            or theta > self.theta_threshold_radians\n",
        "        )\n",
        "\n",
        "        cartx = self.state[0] * 1.5 + 600 / 2.0\n",
        "        if not done:\n",
        "            reward = 1.0 + cartx/1000\n",
        "        elif self.steps_beyond_done is None:\n",
        "            # Pole just fell!\n",
        "            self.steps_beyond_done = 0\n",
        "            reward = 1.0 + cartx/1000\n",
        "        else:\n",
        "            if self.steps_beyond_done == 0:\n",
        "                logger.warn(\n",
        "                    \"You are calling 'step()' even though this \"\n",
        "                    \"environment has already returned done = True. You \"\n",
        "                    \"should always call 'reset()' once you receive 'done = \"\n",
        "                    \"True' -- any further steps are undefined behavior.\"\n",
        "                )\n",
        "            self.steps_beyond_done += 1\n",
        "            reward = 0.0\n",
        "\n",
        "        return np.array(self.state, dtype=np.float32), reward, done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
        "        self.steps_beyond_done = None\n",
        "        return np.array(self.state, dtype=np.float32)\n",
        "\n",
        "    def render(self, mode=\"human\"):\n",
        "        screen_width = 600\n",
        "        screen_height = 400\n",
        "\n",
        "        world_width = self.x_threshold * 2\n",
        "        scale = screen_width / world_width\n",
        "        carty = 100  # TOP OF CART\n",
        "        polewidth = 10.0\n",
        "        polelen = scale * (2 * self.length)\n",
        "        cartwidth = 50.0\n",
        "        cartheight = 30.0\n",
        "\n",
        "        if self.viewer is None:\n",
        "            from gym.envs.classic_control import rendering\n",
        "\n",
        "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
        "            l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2\n",
        "            axleoffset = cartheight / 4.0\n",
        "            cart = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
        "            self.carttrans = rendering.Transform()\n",
        "            cart.add_attr(self.carttrans)\n",
        "            self.viewer.add_geom(cart)\n",
        "            l, r, t, b = (\n",
        "                -polewidth / 2,\n",
        "                polewidth / 2,\n",
        "                polelen - polewidth / 2,\n",
        "                -polewidth / 2,\n",
        "            )\n",
        "            pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
        "            pole.set_color(0.8, 0.6, 0.4)\n",
        "            self.poletrans = rendering.Transform(translation=(0, axleoffset))\n",
        "            pole.add_attr(self.poletrans)\n",
        "            pole.add_attr(self.carttrans)\n",
        "            self.viewer.add_geom(pole)\n",
        "            self.axle = rendering.make_circle(polewidth / 2)\n",
        "            self.axle.add_attr(self.poletrans)\n",
        "            self.axle.add_attr(self.carttrans)\n",
        "            self.axle.set_color(0.5, 0.5, 0.8)\n",
        "            self.viewer.add_geom(self.axle)\n",
        "            self.track = rendering.Line((0, carty), (screen_width, carty))\n",
        "            self.track.set_color(0, 0, 0)\n",
        "            self.viewer.add_geom(self.track)\n",
        "\n",
        "            self._pole_geom = pole\n",
        "\n",
        "        if self.state is None:\n",
        "            return None\n",
        "\n",
        "        # Edit the pole polygon vertex\n",
        "        pole = self._pole_geom\n",
        "        l, r, t, b = (\n",
        "            -polewidth / 2,\n",
        "            polewidth / 2,\n",
        "            polelen - polewidth / 2,\n",
        "            -polewidth / 2,\n",
        "        )\n",
        "        pole.v = [(l, b), (l, t), (r, t), (r, b)]\n",
        "\n",
        "        x = self.state\n",
        "        cartx = x[0] * scale + screen_width / 2.0  # MIDDLE OF CART\n",
        "        self.carttrans.set_translation(cartx, carty)\n",
        "        self.poletrans.set_rotation(-x[2])\n",
        "\n",
        "        return self.viewer.render(return_rgb_array=mode == \"rgb_array\")\n",
        "\n",
        "    def close(self):\n",
        "        if self.viewer:\n",
        "            self.viewer.close()\n",
        "            self.viewer = None"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FesUrycCZlAc",
        "outputId": "1c8acc55-d7de-40dd-a59f-881b472ff2c6"
      },
      "source": [
        "#env = gym.make(\"CartPole-v1\")\n",
        "env=CartPoleEnv()\n",
        "print(env.action_space.n)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEqyFebfZn0l"
      },
      "source": [
        "LEARNING_RATE = 0.1\n",
        "\n",
        "DISCOUNT = 0.95\n",
        "EPISODES = 60000\n",
        "total = 0\n",
        "total_reward = 0\n",
        "prior_reward = 0\n",
        "\n",
        "Observation = [30, 30, 50, 50]\n",
        "np_array_win_size = np.array([0.25, 0.25, 0.01, 0.1])\n",
        "\n",
        "epsilon = 1\n",
        "\n",
        "epsilon_decay_value = 0.99995"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1N2A6vCZsp_",
        "outputId": "81325430-46a6-4a7e-e5cb-b36c02654c9a"
      },
      "source": [
        "q_table = np.random.uniform(low=0, high=1, size=(Observation + [env.action_space.n]))\n",
        "q_table.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30, 30, 50, 50, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPeblKpGZvmy"
      },
      "source": [
        "def get_discrete_state(state):\n",
        "    discrete_state = state/np_array_win_size+ np.array([15,10,1,10])\n",
        "    return tuple(discrete_state.astype(np.int))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGszU8YybLO7"
      },
      "source": [
        "def show_state(env, step=0, info=\"\"):\n",
        "    plt.figure(3)\n",
        "    plt.clf()\n",
        "    plt.imshow(env.render(mode='rgb_array'))\n",
        "    plt.title(\"%s | Step: %d %s\" % (env._spec.id,step, info))\n",
        "    plt.axis('off')\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "    display.display(plt.gcf())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Sad_bZgZ1NE",
        "outputId": "e728a50d-59ae-4483-a80d-3f4f34fbbcb5"
      },
      "source": [
        "for episode in range(EPISODES + 1): #go through the episodes\n",
        "    t0 = time.time() #set the initial time\n",
        "    discrete_state = get_discrete_state(env.reset()) #get the discrete start for the restarted environment \n",
        "    done = False\n",
        "    episode_reward = 0 #reward starts as 0 for each episode\n",
        "\n",
        "    if episode % 2000 == 0: \n",
        "        print(\"Episode: \" + str(episode))\n",
        "\n",
        "    while not done: \n",
        "\n",
        "        if np.random.random() > epsilon:\n",
        "\n",
        "            action = np.argmax(q_table[discrete_state]) #take cordinated action\n",
        "        else:\n",
        "\n",
        "            action = np.random.randint(0, env.action_space.n) #do a random ation\n",
        "\n",
        "        new_state, reward, done, _ = env.step(action) #step action to get new states, reward, and the \"done\" status.\n",
        "\n",
        "        episode_reward += reward #add the reward\n",
        "\n",
        "        new_discrete_state = get_discrete_state(new_state)\n",
        "\n",
        "        #if episode % 2000 == 0: #render\n",
        "            #env.render()\n",
        "\n",
        "        if not done: #update q-table\n",
        "            max_future_q = np.max(q_table[new_discrete_state])\n",
        "\n",
        "            current_q = q_table[discrete_state + (action,)]\n",
        "\n",
        "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "            q_table[discrete_state + (action,)] = new_q\n",
        "\n",
        "        discrete_state = new_discrete_state\n",
        "\n",
        "    if epsilon > 0.05: #epsilon modification\n",
        "        if episode_reward > prior_reward and episode > 10000:\n",
        "            epsilon = math.pow(epsilon_decay_value, episode - 10000)\n",
        "\n",
        "            if episode % 500 == 0:\n",
        "                print(\"Epsilon: \" + str(epsilon))\n",
        "\n",
        "    t1 = time.time() #episode has finished\n",
        "    episode_total = t1 - t0 #episode total time\n",
        "    total = total + episode_total\n",
        "\n",
        "    total_reward += episode_reward #episode total reward\n",
        "    prior_reward = episode_reward\n",
        "\n",
        "    if episode % 1000 == 0: #every 1000 episodes print the average time and the average reward\n",
        "        mean = total / 1000\n",
        "        print(\"Time Average: \" + str(mean))\n",
        "        total = 0\n",
        "\n",
        "        mean_reward = total_reward / 1000\n",
        "        print(\"Mean Reward: \" + str(mean_reward))\n",
        "        total_reward = 0\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0\n",
            "Time Average: 4.686594009399414e-06\n",
            "Mean Reward: 0.028601084616509426\n",
            "Time Average: 0.001291957139968872\n",
            "Mean Reward: 28.38935506217872\n",
            "Episode: 2000\n",
            "Time Average: 0.0013278532028198242\n",
            "Mean Reward: 29.032724627997464\n",
            "Time Average: 0.0013044583797454834\n",
            "Mean Reward: 28.451776290337794\n",
            "Episode: 4000\n",
            "Time Average: 0.0013428418636322022\n",
            "Mean Reward: 29.209774268441343\n",
            "Time Average: 0.0013229768276214599\n",
            "Mean Reward: 29.49434405767757\n",
            "Episode: 6000\n",
            "Time Average: 0.0013236732482910156\n",
            "Mean Reward: 28.523239711939883\n",
            "Time Average: 0.0013107938766479493\n",
            "Mean Reward: 29.593205697356193\n",
            "Episode: 8000\n",
            "Time Average: 0.001300612449645996\n",
            "Mean Reward: 29.196535504489447\n",
            "Time Average: 0.001287632942199707\n",
            "Mean Reward: 28.837726838154\n",
            "Episode: 10000\n",
            "Time Average: 0.0012850377559661865\n",
            "Mean Reward: 28.499901088483508\n",
            "Time Average: 0.00135699462890625\n",
            "Mean Reward: 29.561903118992827\n",
            "Epsilon: 0.9277417467531685\n",
            "Episode: 12000\n",
            "Epsilon: 0.9048351558698463\n",
            "Time Average: 0.0014101979732513428\n",
            "Mean Reward: 30.417579266545292\n",
            "Time Average: 0.0015149459838867188\n",
            "Mean Reward: 33.38674300803466\n",
            "Epsilon: 0.8394533480303666\n",
            "Episode: 14000\n",
            "Epsilon: 0.818726659298009\n",
            "Time Average: 0.0016214404106140137\n",
            "Mean Reward: 35.74488185523482\n",
            "Epsilon: 0.7787959154194878\n",
            "Time Average: 0.0016752352714538574\n",
            "Mean Reward: 38.19549905462173\n",
            "Epsilon: 0.7595669010105212\n",
            "Episode: 16000\n",
            "Time Average: 0.001810810089111328\n",
            "Mean Reward: 40.43127926837474\n",
            "Epsilon: 0.7225214829355084\n",
            "Epsilon: 0.7046819235193919\n",
            "Time Average: 0.0019853010177612305\n",
            "Mean Reward: 43.03931186748864\n",
            "Episode: 18000\n",
            "Epsilon: 0.6703133426452782\n",
            "Time Average: 0.002201900005340576\n",
            "Mean Reward: 47.14933125681135\n",
            "Time Average: 0.0022741496562957765\n",
            "Mean Reward: 50.85192195505235\n",
            "Episode: 20000\n",
            "Epsilon: 0.606523077874078\n",
            "Time Average: 0.002511436223983765\n",
            "Mean Reward: 55.92599479960339\n",
            "Epsilon: 0.5915475999948323\n",
            "Time Average: 0.0026921021938323977\n",
            "Mean Reward: 60.115432277756206\n",
            "Epsilon: 0.5626967797130051\n",
            "Episode: 22000\n",
            "Time Average: 0.0029578304290771486\n",
            "Mean Reward: 64.59399768214953\n",
            "Epsilon: 0.5352530648457575\n",
            "Time Average: 0.0034168994426727294\n",
            "Mean Reward: 73.50795532845088\n"
          ]
        }
      ]
    }
  ]
}