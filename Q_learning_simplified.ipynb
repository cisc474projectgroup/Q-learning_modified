{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q-learning_simplified.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRCMwG_EZgoV"
      },
      "source": [
        "#https://medium.com/swlh/using-q-learning-for-openais-cartpole-v1-4a216ef237df\n",
        "#source from \n",
        "#A extension on the project\n",
        "import numpy as np # used for arrays\n",
        "\n",
        "import gym # pull the environment\n",
        "\n",
        "import time # to get the time\n",
        "\n",
        "import math # needed for calculations\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython import display"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DmI9SBmcn5r"
      },
      "source": [
        "import pyvirtualdisplay\n",
        "\n",
        "\n",
        "_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
        "                                    size=(1400, 900))\n",
        "_ = _display.start()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FesUrycCZlAc",
        "outputId": "43342395-bd51-455b-fcfe-4012c4d5d72c"
      },
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "print(env.action_space.n)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEqyFebfZn0l"
      },
      "source": [
        "LEARNING_RATE = 0.1\n",
        "\n",
        "DISCOUNT = 0.95\n",
        "EPISODES = 60000\n",
        "total = 0\n",
        "total_reward = 0\n",
        "prior_reward = 0\n",
        "\n",
        "Observation = [30, 30, 50, 50]\n",
        "np_array_win_size = np.array([0.25, 0.25, 0.01, 0.1])\n",
        "\n",
        "epsilon = 1\n",
        "\n",
        "epsilon_decay_value = 0.99995"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1N2A6vCZsp_",
        "outputId": "3878ffd0-2709-4400-e442-34cf47adb99c"
      },
      "source": [
        "q_table = np.random.uniform(low=0, high=1, size=(Observation + [env.action_space.n]))\n",
        "q_table.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30, 30, 50, 50, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPeblKpGZvmy"
      },
      "source": [
        "def get_discrete_state(state):\n",
        "    discrete_state = state/np_array_win_size+ np.array([15,10,1,10])\n",
        "    return tuple(discrete_state.astype(np.int))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGszU8YybLO7"
      },
      "source": [
        "def show_state(env, step=0, info=\"\"):\n",
        "    plt.figure(3)\n",
        "    plt.clf()\n",
        "    plt.imshow(env.render(mode='rgb_array'))\n",
        "    plt.title(\"%s | Step: %d %s\" % (env._spec.id,step, info))\n",
        "    plt.axis('off')\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "    display.display(plt.gcf())"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3Sad_bZgZ1NE",
        "outputId": "9f60ff6f-2fd3-48d6-8614-e106c15b0994"
      },
      "source": [
        "for episode in range(EPISODES + 1): #go through the episodes\n",
        "    t0 = time.time() #set the initial time\n",
        "    discrete_state = get_discrete_state(env.reset()) #get the discrete start for the restarted environment \n",
        "    done = False\n",
        "    episode_reward = 0 #reward starts as 0 for each episode\n",
        "\n",
        "    if episode % 2000 == 0: \n",
        "        print(\"Episode: \" + str(episode))\n",
        "\n",
        "    while not done: \n",
        "\n",
        "        if np.random.random() > epsilon:\n",
        "\n",
        "            action = np.argmax(q_table[discrete_state]) #take cordinated action\n",
        "        else:\n",
        "\n",
        "            action = np.random.randint(0, env.action_space.n) #do a random ation\n",
        "\n",
        "        new_state, reward, done, _ = env.step(action) #step action to get new states, reward, and the \"done\" status.\n",
        "\n",
        "        episode_reward += reward #add the reward\n",
        "\n",
        "        new_discrete_state = get_discrete_state(new_state)\n",
        "\n",
        "        #if episode % 2000 == 0: #render\n",
        "            #env.render()\n",
        "\n",
        "        if not done: #update q-table\n",
        "            max_future_q = np.max(q_table[new_discrete_state])\n",
        "\n",
        "            current_q = q_table[discrete_state + (action,)]\n",
        "\n",
        "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "            q_table[discrete_state + (action,)] = new_q\n",
        "\n",
        "        discrete_state = new_discrete_state\n",
        "\n",
        "    if epsilon > 0.05: #epsilon modification\n",
        "        if episode_reward > prior_reward and episode > 10000:\n",
        "            epsilon = math.pow(epsilon_decay_value, episode - 10000)\n",
        "\n",
        "            if episode % 500 == 0:\n",
        "                print(\"Epsilon: \" + str(epsilon))\n",
        "\n",
        "    t1 = time.time() #episode has finished\n",
        "    episode_total = t1 - t0 #episode total time\n",
        "    total = total + episode_total\n",
        "\n",
        "    total_reward += episode_reward #episode total reward\n",
        "    prior_reward = episode_reward\n",
        "\n",
        "    if episode % 1000 == 0: #every 1000 episodes print the average time and the average reward\n",
        "        mean = total / 1000\n",
        "        print(\"Time Average: \" + str(mean))\n",
        "        total = 0\n",
        "\n",
        "        mean_reward = total_reward / 1000\n",
        "        print(\"Mean Reward: \" + str(mean_reward))\n",
        "        total_reward = 0\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0\n",
            "Time Average: 2.5198459625244143e-06\n",
            "Mean Reward: 0.016\n",
            "Time Average: 0.0013300015926361083\n",
            "Mean Reward: 22.272\n",
            "Episode: 2000\n",
            "Time Average: 0.0013951184749603272\n",
            "Mean Reward: 22.635\n",
            "Time Average: 0.0013160240650177003\n",
            "Mean Reward: 22.296\n",
            "Episode: 4000\n",
            "Time Average: 0.0012665488719940186\n",
            "Mean Reward: 22.118\n",
            "Time Average: 0.0013047440052032472\n",
            "Mean Reward: 21.842\n",
            "Episode: 6000\n",
            "Time Average: 0.0013705005645751954\n",
            "Mean Reward: 22.792\n",
            "Time Average: 0.0013414056301116943\n",
            "Mean Reward: 22.999\n",
            "Episode: 8000\n",
            "Time Average: 0.0013695015907287597\n",
            "Mean Reward: 21.805\n",
            "Time Average: 0.001259756088256836\n",
            "Mean Reward: 22.175\n",
            "Episode: 10000\n",
            "Time Average: 0.0013080518245697022\n",
            "Mean Reward: 22.718\n",
            "Time Average: 0.0013596465587615966\n",
            "Mean Reward: 22.715\n",
            "Episode: 12000\n",
            "Time Average: 0.0013784351348876953\n",
            "Mean Reward: 23.797\n",
            "Epsilon: 0.8824941446941661\n",
            "Epsilon: 0.8607047486686201\n",
            "Time Average: 0.0015139980316162109\n",
            "Mean Reward: 25.225\n",
            "Episode: 14000\n",
            "Time Average: 0.0015770161151885986\n",
            "Mean Reward: 27.192\n",
            "Epsilon: 0.7985117269685725\n",
            "Epsilon: 0.7787959154194878\n",
            "Time Average: 0.001775576114654541\n",
            "Mean Reward: 29.82\n",
            "Epsilon: 0.7595669010105212\n",
            "Episode: 16000\n",
            "Epsilon: 0.7408126643807126\n",
            "Time Average: 0.001821408748626709\n",
            "Mean Reward: 30.725\n",
            "Time Average: 0.0019780492782592776\n",
            "Mean Reward: 34.243\n",
            "Episode: 18000\n",
            "Time Average: 0.002086430311203003\n",
            "Mean Reward: 35.634\n",
            "Time Average: 0.0022901642322540284\n",
            "Mean Reward: 38.681\n",
            "Episode: 20000\n",
            "Time Average: 0.0025621697902679444\n",
            "Mean Reward: 42.268\n",
            "Epsilon: 0.5915475999948323\n",
            "Epsilon: 0.5769418771107269\n",
            "Time Average: 0.0029223823547363283\n",
            "Mean Reward: 47.772\n",
            "Epsilon: 0.5626967797130051\n",
            "Episode: 22000\n",
            "Epsilon: 0.5488034037068503\n",
            "Time Average: 0.0029785428047180177\n",
            "Mean Reward: 49.869\n",
            "Epsilon: 0.5352530648457575\n",
            "Epsilon: 0.5220372933033263\n",
            "Time Average: 0.003363292694091797\n",
            "Mean Reward: 54.757\n",
            "Episode: 24000\n",
            "Epsilon: 0.4965766133349901\n",
            "Time Average: 0.0035053720474243163\n",
            "Mean Reward: 59.534\n",
            "Epsilon: 0.47235769565598784\n",
            "Time Average: 0.0036943576335906982\n",
            "Mean Reward: 62.482\n",
            "Episode: 26000\n",
            "Time Average: 0.004177479028701782\n",
            "Mean Reward: 70.048\n",
            "Epsilon: 0.43822595366018774\n",
            "Epsilon: 0.4274058491752072\n",
            "Time Average: 0.004363074541091919\n",
            "Mean Reward: 73.989\n",
            "Epsilon: 0.41685290061763824\n",
            "Episode: 28000\n",
            "Time Average: 0.0048349864482879635\n",
            "Mean Reward: 80.947\n",
            "Epsilon: 0.396522249086328\n",
            "Time Average: 0.004944210529327393\n",
            "Mean Reward: 83.878\n",
            "Episode: 30000\n",
            "Epsilon: 0.3678702439938449\n",
            "Time Average: 0.0050277419090271\n",
            "Mean Reward: 85.951\n",
            "Epsilon: 0.3587872710578896\n",
            "Time Average: 0.005504690408706665\n",
            "Mean Reward: 92.779\n",
            "Episode: 32000\n",
            "Time Average: 0.005347263813018799\n",
            "Mean Reward: 90.857\n",
            "Time Average: 0.005952088832855224\n",
            "Mean Reward: 102.808\n",
            "Epsilon: 0.30880990796138097\n",
            "Episode: 34000\n",
            "Epsilon: 0.3011851759202241\n",
            "Time Average: 0.006449207782745361\n",
            "Mean Reward: 110.491\n",
            "Time Average: 0.00670479679107666\n",
            "Mean Reward: 114.601\n",
            "Epsilon: 0.27942206120438906\n",
            "Episode: 36000\n",
            "Epsilon: 0.27252293559946306\n",
            "Time Average: 0.006723278045654297\n",
            "Mean Reward: 118.055\n",
            "Epsilon: 0.25923151114313064\n",
            "Time Average: 0.007293621301651001\n",
            "Mean Reward: 126.02\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-31b086b8ef3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#do a random ation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#step action to get new states, reward, and the \"done\" status.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;31m#add the reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%r (%s) invalid\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_dot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_dot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/spaces/discrete.py\u001b[0m in \u001b[0;36mcontains\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mas_int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}